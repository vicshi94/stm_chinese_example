{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 论坛文本数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "from zhon.hanzi import punctuation\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "info = pd.read_csv('../data0629/info_immigration.csv',encoding='utf-8-sig')\n",
    "# cmt = pd.read_csv('../data0629/cmt_immigration.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_content(text):\n",
    "    if text == ' ':\n",
    "        return ' '\n",
    "    # remove the system info '[...]'\n",
    "    new_content = re.sub(r'\\[[\\n][^\\]]*[\\n]\\]','',text)\n",
    "    # remove the \\n\n",
    "    new_content = new_content.replace('\\n',' ')\n",
    "    # remove the url\n",
    "    new_content = re.sub(r'http[s]?://[^\\s]*','',new_content)\n",
    "    # remove the .jpg/.jpeg\n",
    "    new_content = re.sub(r'[\\w]*\\.[a-zA-Z]{3,4}','',new_content)\n",
    "    # remove the (xxx.xx KB) or (xx.xx MB)\n",
    "    new_content = re.sub(r'\\([\\d]*\\.[\\d]* [a-zA-Z]{2}\\)','',new_content)\n",
    "    \n",
    "    # 97 is a special case\n",
    "    new_content = new_content.replace('97','九七')\n",
    "    # remove the timestamp\n",
    "    new_content = re.sub(r'\\d{2,4}-\\d{1,2}-\\d{1,2} \\d{2}:\\d{2}:\\d{2}','',new_content)\n",
    "    new_content = re.sub(r'\\d{2,4}-\\d{1,2}-\\d{1,2} \\d{2}:\\d{2}','',new_content)\n",
    "    new_content = re.sub(r'\\d{2,4}-\\d{1,2}-\\d{1,2}','',new_content)\n",
    "    new_content = re.sub(r'\\d{2,4}年\\d{1,2}月\\d{1,2}日','',new_content)\n",
    "    new_content = re.sub(r'\\d{2,4}年\\d{1,2}月','',new_content)\n",
    "    new_content = re.sub(r'\\d{2,4}年','',new_content)\n",
    "    new_content = re.sub(r'\\d{1,2}月\\d{1,2}日','',new_content)\n",
    "    new_content = re.sub(r'\\d{1,2}月','',new_content)\n",
    "    new_content = re.sub(r'\\d{1,2}日','',new_content)\n",
    "    new_content = re.sub(r'\\d{2}:\\d{2}:\\d{2}','',new_content)\n",
    "    new_content = re.sub(r'\\d{2}:\\d{2}','',new_content)\n",
    "    # remove the punctuation marks\n",
    "    new_content = re.sub(r'[%s]+' %punctuation,' ',new_content)\n",
    "    # remove the , and .\n",
    "    new_content = new_content.replace(',',' ')\n",
    "    new_content = new_content.replace('.',' ')\n",
    "    new_content = new_content.replace('(',' ')\n",
    "    new_content = new_content.replace(')',' ')\n",
    "    new_content = new_content.replace('=',' ')\n",
    "    new_content = new_content.replace('?',' ')\n",
    "    new_content = new_content.replace('/',' ')\n",
    "    new_content = new_content.replace(':',' ')\n",
    "    new_content = new_content.replace('-',' ')\n",
    "\n",
    "    # remove the number\n",
    "    new_content = re.sub(r'\\d+','',new_content)\n",
    "\n",
    "    return new_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info-level\n",
    "info['fulltext'] = info['fulltext'].apply(lambda x:re_content(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmt-level\n",
    "cmt['quote_content'].fillna(' ',inplace=True)\n",
    "cmt['cmt_content'].fillna(' ',inplace=True)\n",
    "# replace'此回覆已被刪除' with ' '\n",
    "cmt['quote_content'] = cmt['quote_content'].apply(lambda x:x.replace('此回覆已被刪除',' '))\n",
    "cmt['cmt_content'] = cmt['cmt_content'].apply(lambda x:x.replace('此回覆已被刪除',' '))\n",
    "# clean the data\n",
    "cmt['quote_content'] = cmt['quote_content'].apply(lambda x:re_content(x))\n",
    "cmt['cmt_content'] = cmt['cmt_content'].apply(lambda x:re_content(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "CyberCan = pd.read_excel('./CyberCan.xlsx')\n",
    "for i in range(len(CyberCan)):\n",
    "    jieba.add_word(str(CyberCan['Words'][i]),freq=CyberCan['Frequency'][i])\n",
    "\n",
    "jieba.add_word('Hong Kong',freq=100000)\n",
    "jieba.add_word('九七',freq=20000)\n",
    "jieba.add_word('等於',freq=20000)\n",
    "jieba.add_word('由己',freq=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_dict = pd.read_excel('../pre_describe/immigration_dictionary_v1_0527.xlsx',skiprows=2,usecols=['Word'])\n",
    "# add the words in the dictionary to jieba\n",
    "for i in range(len(immigration_dict)):\n",
    "    if immigration_dict['Word'][i] not in CyberCan['Words'].values:\n",
    "        jieba.add_word(immigration_dict['Word'][i],freq=18883)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "stop_words = [' ','的','嘅','在','有','是','和','係','我','及','都','但','他','就','與','喺','為','對','要','到','而','了','被','你','會','又','同','或','也','亦','她','佢']\n",
    "\n",
    "# # check the high frequency words and tune the stop words\n",
    "# info_content_seg = []\n",
    "# for i in info['fulltext']:\n",
    "#     seg = jieba.lcut(i)\n",
    "#     info_content_seg.append(seg)\n",
    "\n",
    "# contents_clean = []\n",
    "# for wl in info_content_seg:\n",
    "#     wl_clean = []\n",
    "#     for w in wl:\n",
    "#         if w not in stop_words:\n",
    "#             wl_clean.append(w)\n",
    "#     contents_clean.append(wl_clean)\n",
    "\n",
    "# all_words = []\n",
    "# for wl in contents_clean:\n",
    "#     all_words += wl\n",
    "\n",
    "# counter = Counter(all_words)\n",
    "# counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正式分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_words(text):\n",
    "    seg = jieba.lcut(text)\n",
    "    new_text = ' '.join(seg)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['fulltext'] = info['fulltext'].apply(lambda x:seg_words(x))\n",
    "# cmt['quote_content'] = cmt['quote_content'].apply(lambda x:seg_words(x))\n",
    "# cmt['cmt_content'] = cmt['cmt_content'].apply(lambda x:seg_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_hotness = pd.read_csv('../data0629/info_hotness_immigration.csv')\n",
    "# cmt_hotness = pd.read_csv('../data0629/cmt_hotness_immigration.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the info and info_hotness\n",
    "info = pd.merge(info,info_hotness,on='info_id',how='left')\n",
    "# combine the cmt and cmt_hotness\n",
    "# cmt = pd.merge(cmt,cmt_hotness,on='cmt_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = info[['platform_x','publish_time','info_id','user_id','title','fulltext','contain_url','view_count', 'like_count', 'dislike_count',\n",
    "       'cmt_count', 'repost_count']]\n",
    "info.rename(columns={'platform_x':'platform'},inplace=True)\n",
    "info['view_count'] = info['view_count'].fillna(0).astype(int)\n",
    "info['like_count'] = info['like_count'].fillna(0).astype(int)\n",
    "info['dislike_count'] = info['dislike_count'].fillna(0).astype(int)\n",
    "info['cmt_count'] = info['cmt_count'].fillna(0).astype(int)\n",
    "info['repost_count'] = info['repost_count'].fillna(0).astype(int)\n",
    "info.to_csv('../data0629/info_processed_1029.csv',index=False,encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt = cmt[['platform_x', 'info_id', 'cmt_time', 'cmt_id', 'cmt_user_id','cmt_tgt_user_id',\n",
    "       'have_quote', 'quote_content', 'cmt_content', 'cmt_at_user', 'cmt_contain_url', \n",
    "       'like_count', 'dislike_count', 'reply_count','repost_count']]\n",
    "cmt.rename(columns={'platform_x':'platform'},inplace=True)\n",
    "cmt['cmt_id'] = cmt['cmt_id'].astype(str)\n",
    "# turn the cmt_user_id to int and ignore the nan, then save as string\n",
    "cmt['cmt_user_id'] = cmt['cmt_user_id'].dropna().astype(int)\n",
    "cmt['cmt_user_id'] = cmt['cmt_user_id'].astype(str)\n",
    "cmt['cmt_user_id'] = cmt['cmt_user_id'].apply(lambda x:x.replace('nan',''))\n",
    "\n",
    "cmt['have_quote'] = cmt['have_quote'].fillna(0).astype(int)\n",
    "cmt['like_count'] = cmt['like_count'].fillna(0).astype(int)\n",
    "cmt['dislike_count'] = cmt['dislike_count'].fillna(0).astype(int)\n",
    "cmt['reply_count'] = cmt['reply_count'].fillna(0).astype(int)\n",
    "cmt['repost_count'] = cmt['repost_count'].fillna(0).astype(int)\n",
    "\n",
    "cmt.to_csv('../data0629/cmt_processed1029.csv',index=False,encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIWC结果合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_csv('../data0629/info_processed_1029.csv',encoding='utf-8-sig')\n",
    "info_LIWC1 = pd.read_csv('../data0629/info_processed_LIWC1_1030.csv')\n",
    "info_LIWC2 = pd.read_csv('../data0629/info_processed_LIWC2_1030.csv',usecols=['info_id','WC', 'posemo', 'negemo', 'anx', 'anger','sad','swear'])\n",
    "\n",
    "info = pd.merge(info,info_LIWC1,on='info_id',how='left')\n",
    "info = pd.merge(info,info_LIWC2,on='info_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['Stance'] = info['Localists']-info['支持大陸']\n",
    "\n",
    "# normalize the stance to -1~1\n",
    "info['Stance'] = info['Stance'].apply(lambda x: x/(info['Localists'].max()-info['Localists'].min()))\n",
    "\n",
    "# Stage division\n",
    "# Stage 0: before 2020-6-30\n",
    "# Stage 1: 2020-6-30 to 2021-1-5\n",
    "# Stage 2: 2021-1-6 to 2021-2-7\n",
    "# Stage 3: 2021-2-8 to 2021-6-16\n",
    "# Stage 4: 2021-6-17 to 2021-8-12\n",
    "# Stage 5: 2021-8-13 to 2021-10-28\n",
    "# Stage 6: 2021-10-29 to 2022-2-20\n",
    "# Stage 7: 2022-2-21 to present\n",
    "info['stage'] = info['publish_time'].apply(lambda x:0 if x<'2020-06-30' else 1 if x<'2021-01-06' else 2 if x<'2021-02-08' else 3 if x<'2021-06-17' else 4 if x<'2021-08-13' else 5 if x<'2021-10-29' else 6 if x<'2022-02-21' else 7)\n",
    "info.to_csv('../data0629/info_afterLIWC_1030.csv',index=False,encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_stm = info.copy()\n",
    "\n",
    "# log trans the WC\n",
    "info_stm['WC'] = info_stm['WC'].apply(lambda x:np.log(x))\n",
    "\n",
    "# senti\n",
    "info_stm['senti'] = info_stm['posemo']-info_stm['negemo']\n",
    "info_stm['senti'] = info_stm['senti'].apply(lambda x: x/(info_stm['posemo'].max()))\n",
    "\n",
    "info_stm = info_stm[['platform', 'info_id', 'stage', 'fulltext', 'WC','senti','anx','anger','sad','swear','攻擊型','Stance']]\n",
    "info_stm = info_stm[info_stm['stage']>0]\n",
    "info_stm['stage'] = info_stm['stage'].apply(lambda x:'Stage'+str(x))\n",
    "info_stm.to_csv('../data0629/info_stm_1030.csv',index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to xlsx: 保存为xlxs最好\n",
    "import xlsxwriter\n",
    "\n",
    "info_stm.to_excel('../data0629/info_stm_1030.xlsx',index=False, engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_copy = info[['platform','info_id','fulltext']]\n",
    "# info_copy.rename(columns={'fulltext':'text'},inplace=True)\n",
    "# info_copy['level'] = 'info'\n",
    "# cmt_copy = cmt[['platform','info_id','cmt_content']]\n",
    "# cmt_copy.rename(columns={'cmt_content':'text'},inplace=True)\n",
    "# cmt_copy['level'] = 'cmt'\n",
    "\n",
    "# # combine the info and cmt\n",
    "# stm_data = pd.concat([info_copy,cmt_copy],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stm_data.to_csv('../data0629/stm_data1027.csv',index=False,encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
